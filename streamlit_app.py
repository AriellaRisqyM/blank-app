# ==============================================================================
# Aplikasi Streamlit: Analisis Sentimen (Lexicon + ML)
# ==============================================================================
import streamlit as st
import pandas as pd
import requests
import json
import re
import nltk
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from tqdm.auto import tqdm
import io

# ML Imports
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

# ! ! ! IMPORT BARU UNTUK WORDCLOUD ! ! !
from wordcloud import WordCloud

# Configure tqdm for Streamlit
tqdm.pandas()

# ==============================================================================
# Cache Resource Loading Functions (Dictionaries, Models)
# ==============================================================================
@st.cache_resource
def load_kamus_normalisasi(url):
Â  Â  st.info(f"Mengunduh kamus normalisasi...")
Â  Â  try:
Â  Â  Â  Â  response = requests.get(url); response.raise_for_status(); kamus = response.json()
Â  Â  Â  Â  st.success(f"Ok: {len(kamus)} kata normalisasi.")
Â  Â  Â  Â  return kamus
Â  Â  except Exception as e: st.error(f"Gagal: {e}"); return {}
@st.cache_resource
def load_combined_stopwords(url_kamus_online):
Â  Â  st.info("Memuat stopwords..."); sw_sastrawi=set(); sw_kamus=set()
Â  Â  try: factory=StopWordRemoverFactory(); sw_sastrawi=set(factory.get_stop_words()); st.success(f"Ok: {len(sw_sastrawi)} stopwords Sastrawi.")
Â  Â  except Exception as e: st.warning(f"Sastrawi gagal: {e}.")
Â  Â  try: response=requests.get(url_kamus_online); response.raise_for_status(); sw_kamus=set(response.text.splitlines()); st.success(f"Ok: {len(sw_kamus)} stopwords online.")
Â  Â  except Exception as e: st.warning(f"Stopwords online gagal: {e}.")
Â  Â  combined = sw_sastrawi.union(sw_kamus); st.success(f"Total stopwords: {len(combined)}."); return combined
@st.cache_resource
def get_stemmer(): st.info("Inisialisasi stemmer..."); factory=StemmerFactory(); stemmer=factory.create_stemmer(); st.success("Stemmer siap."); return stemmer
@st.cache_resource
def load_tsv_dict(url, key_col, val_col, header=None, sep='\t'):
Â  Â  try:
Â  Â  Â  Â  df=pd.read_csv(url, sep=sep, header=header, names=[key_col, val_col], on_bad_lines='skip', engine='python'); df[val_col]=df[val_col].astype(str).str.replace(r'[+]', '', regex=True)
Â  Â  Â  Â  valid_rows=pd.to_numeric(df[val_col], errors='coerce').notna(); df_valid=df[valid_rows]; return dict(zip(df_valid[key_col], df_valid[val_col].astype(int)))
Â  Â  except Exception as e: st.warning(f"Gagal TSV {url.split('/')[-1]}: {e}"); return {}
@st.cache_resource
def load_manual_dict(url):
Â  Â  dictionary={};
Â  Â  try:
Â  Â  Â  Â  r=requests.get(url); r.raise_for_status(); lines=r.text.splitlines()
Â  Â  Â  Â  for line in lines:
Â  Â  Â  Â  Â  Â  line=line.strip();
Â  Â  Â  Â  Â  Â  if not line or line.startswith(('word ', 'phrase ', 'emo ')): continue
Â  Â  Â  Â  Â  Â  parts=line.rsplit(' ', 1);
Â  Â  Â  Â  Â  Â  if len(parts)==2:
Â  Â  Â  Â  Â  Â  Â  Â  key, val=parts;
Â  Â  Â  Â  Â  Â  Â  Â  try: score=int(val.strip().replace('+', '')); dictionary[key.strip()]=score
Â  Â  Â  Â  Â  Â  Â  Â  except ValueError: pass
Â  Â  Â  Â  return dictionary
Â  Â  except Exception as e: st.warning(f"Gagal manual {url.split('/')[-1]}: {e}"); return {}
@st.cache_resource
def load_set(url):
Â  Â  try: r=requests.get(url); r.raise_for_status(); return set(line.strip() for line in r.text.splitlines() if line.strip())
Â  Â  except Exception as e: st.warning(f"Gagal set {url.split('/')[-1]}: {e}"); return set()
@st.cache_resource
def load_all_sentiment_dictionaries(urls):
Â  Â  st.info("Memuat semua kamus sentimen..."); senti_dict={}
Â  Â  senti_dict.update(load_tsv_dict(urls['inset_fajri_pos'], 'word', 'score', header=None, sep='\t'))
Â  Â  senti_dict.update(load_tsv_dict(urls['inset_fajri_neg'], 'word', 'score', header=None, sep='\t'))
Â  Â  senti_dict.update(load_tsv_dict(urls['inset_onpilot_pos'], 'word', 'weight', header=0, sep='\t'))
Â  Â  senti_dict.update(load_tsv_dict(urls['inset_onpilot_neg'], 'word', 'weight', header=0, sep='\t'))
Â  Â  try: r=requests.get(urls['senti_json']); r.raise_for_status(); senti_dict.update(r.json()); st.info("Ok: SentiStrength JSON.")
Â  Â  except Exception as e: st.warning(f"Gagal Senti JSON: {e}")
Â  Â  senti_dict.update(load_manual_dict(urls['emoticon'])); senti_dict.update(load_manual_dict(urls['booster']))
Â  Â  idiom_dict=load_manual_dict(urls['idiom']); sorted_idioms=sorted(idiom_dict.items(), key=lambda x: len(x[0]), reverse=True)
Â  Â  negating_words=load_set(urls['negating']); question_words=load_set(urls['question'])
Â  Â  st.success(f"Ok: Kamus utama ({len(senti_dict)}), idiom ({len(idiom_dict)}), negasi ({len(negating_words)}), tanya ({len(question_words)}).")
Â  Â  return senti_dict, sorted_idioms, negating_words, question_words


# ==============================================================================
# Preprocessing Functions
# ==============================================================================
def clean_text(text):
Â  Â  if not isinstance(text, str): return ""
Â  Â  text = re.sub(r'http\S+', '', text); text = re.sub(r'@\w+', '', text); text = re.sub(r'#', ' ', text)
Â  Â  text = re.sub(r'[^a-zA-Z\s]', '', text); text = re.sub(r'\s+', ' ', text).strip(); return text

def normalize_tokens(tokens, kamus_normalisasi):
Â  Â  if not isinstance(tokens, list): return []
Â  Â  return [kamus_normalisasi.get(word, word) for word in tokens]

def remove_stopwords(tokens, stop_words):
Â  Â  if not isinstance(tokens, list): return []
Â  Â  return [word for word in tokens if word not in stop_words and len(word) > 1]

def stem_tokens(tokens, _stemmer):
Â  Â  if not isinstance(tokens, list): return []
Â  Â  return [stemmed for stemmed in [_stemmer.stem(word) for word in tokens] if stemmed]


# ==============================================================================
# "Canggih" Labeling Function
# ==============================================================================
def label_sentiment_canggih(text, senti_dict, sorted_idioms, negating_words, question_words):
Â  Â  if not isinstance(text, str) or not text.strip(): return 'netral'
Â  Â  original_text_lower = text.lower(); pos_score = 0; neg_score = 0
Â  Â  words_for_q_check = original_text_lower.split()
Â  Â  if any(q_word in words_for_q_check for q_word in question_words): return 'netral'
Â  Â  text_after_idioms = original_text_lower; processed_idiom = False
Â  Â  for idiom, score in sorted_idioms:
Â  Â  Â  Â  if f" {idiom} " in f" {text_after_idioms} ":
Â  Â  Â  Â  Â  Â  if score > 0: pos_score += score
Â  Â  Â  Â  Â  Â  else: neg_score += abs(score)
Â  Â  Â  Â  Â  Â  text_after_idioms = text_after_idioms.replace(idiom, " "); processed_idiom = True
Â  Â  words = text_after_idioms.split() if processed_idiom else words_for_q_check
Â  Â  is_negated = False
Â  Â  for i, word in enumerate(words):
Â  Â  Â  Â  if not word: continue
Â  Â  Â  Â  if word in negating_words: is_negated = True; continue
Â  Â  Â  Â  score = senti_dict.get(word, 0)
Â  Â  Â  Â  if score != 0:
Â  Â  Â  Â  Â  Â  if is_negated: score *= -1
Â  Â  Â  Â  Â  Â  if i > 0 and words[i-1] in senti_dict:
Â  Â  Â  Â  Â  Â  Â  Â  Â booster_score = senti_dict.get(words[i-1], 0)
Â  Â  Â  Â  Â  Â  Â  Â  Â if booster_score in [-2, -1, 1, 2]:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â if score > 0: score += booster_score
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â elif score < 0: score -= booster_score
Â  Â  Â  Â  Â  Â  if score > 0: pos_score += score
Â  Â  Â  Â  Â  Â  else: neg_score += abs(score)
Â  Â  Â  Â  Â  Â  is_negated = False
Â  Â  if pos_score == 0 and neg_score == 0: return 'netral'
Â  Â  if pos_score > neg_score * 1.5: return 'positif'
Â  Â  elif neg_score > pos_score * 1.5: return 'negatif'
Â  Â  else: return 'netral'

# ==============================================================================
# Preprocessing + Labeling Function (Cacheable)
# ==============================================================================
@st.cache_data
def preprocess_and_label_df(_df, text_column, _senti_dict, _sorted_idioms, _negating_words, _question_words):
Â  Â  """Applies cleaning, case folding, and lexicon labeling."""
Â  Â  st.info("Memulai preprocessing & pelabelan lexicon...")
Â  Â  df_processed = _df.copy() # Use _df to avoid conflict with streamlit elements

Â  Â  # Apply Cleaning & Case Folding
Â  Â  st.write("1. Cleaning & Case Folding...")
Â  Â  df_processed['cleaned_text'] = df_processed[text_column].apply(clean_text)
Â  Â  df_processed.dropna(subset=['cleaned_text'], inplace=True)
Â  Â  df_processed = df_processed[df_processed['cleaned_text'].str.strip().astype(bool)]
Â  Â  if df_processed.empty:
Â  Â  Â  Â  st.warning("Tidak ada teks valid setelah cleaning.")
Â  Â  Â  Â  return df_processed # Return empty df
Â  Â  df_processed['case_folded_text'] = df_processed['cleaned_text'].str.lower()

Â  Â  # Apply "Canggih" Labeling on 'case_folded_text'
Â  Â  st.write("2. Pelabelan Sentimen (Lexicon Canggih)...")
Â  Â  # Make sure to pass all required dictionaries
Â  Â  df_processed['sentiment'] = df_processed['case_folded_text'].progress_apply(
Â  Â  Â  Â  lambda text: label_sentiment_canggih(text, _senti_dict, _sorted_idioms, _negating_words, _question_words)
Â  Â  )

Â  Â  st.success("Preprocessing & Pelabelan Lexicon Selesai.")
Â  Â  # Return df with key columns first
Â  Â  cols_to_return = [text_column, 'case_folded_text', 'sentiment'] + [col for col in df_processed.columns if col not in [text_column, 'case_folded_text', 'sentiment', 'cleaned_text']]
Â  Â  return df_processed[cols_to_return]

# ==============================================================================
# ML Modeling Function (Not Cached for dynamic training)
# ==============================================================================
def train_and_evaluate_models(df_labeled, selected_test_size=0.3, text_col_for_tfidf='case_folded_text', target_col='sentiment'):
Â  Â  """Performs Train/Test split, TF-IDF, trains NB & SVM, returns results."""
Â  Â  st.info("Memulai persiapan data dan pelatihan model ML...")

Â  Â  if df_labeled.empty or target_col not in df_labeled.columns or text_col_for_tfidf not in df_labeled.columns:
Â  Â  Â  Â  st.error("Dataframe input kosong atau kolom yang dibutuhkan tidak ada.")
Â  Â  Â  Â  return None

Â  Â  # Pastikan tidak ada NaN di kolom teks atau target
Â  Â  df_labeled = df_labeled.dropna(subset=[text_col_for_tfidf, target_col])
Â  Â  if len(df_labeled[target_col].unique()) < 2:
Â  Â  Â  Â  Â st.error(f"Hanya ada {len(df_labeled[target_col].unique())} kelas unik di kolom target '{target_col}'. Minimal 2 kelas dibutuhkan untuk pelatihan.")
Â  Â  Â  Â  Â return None
Â  Â  if len(df_labeled) < 10: # Arbitrary small number
Â  Â  Â  Â  Â st.warning(f"Jumlah data ({len(df_labeled)}) sangat sedikit, hasil model mungkin tidak reliable.")


Â  Â  X = df_labeled[text_col_for_tfidf]
Â  Â  y = df_labeled[target_col]

Â  Â  # Split Data (Stratify to handle imbalance)
Â  Â  try:
Â  Â  Â  Â  X_train, X_test, y_train, y_test = train_test_split(
Â  Â  Â  Â  Â  Â  X, y,
Â  Â  Â  Â  Â  Â  test_size=selected_test_size, # <-- Menggunakan nilai dari slider
Â  Â  Â  Â  Â  Â  random_state=42,
Â  Â  Â  Â  Â  Â  stratify=y
Â  Â  Â  Â  )
Â  Â  Â  Â Â 
Â  Â  Â  Â  st.write(f"Data dibagi: {len(X_train)} train, {len(X_test)} test (Test Size = {selected_test_size})")
Â  Â  Â  Â  st.write("Distribusi kelas (Train):", y_train.value_counts())
Â  Â  Â  Â  st.write("Distribusi kelas (Test):", y_test.value_counts())
Â  Â  Â  Â Â 
Â  Â  except ValueError as e:
Â  Â  Â  Â  Â st.error(f"Gagal membagi data (mungkin karena data terlalu sedikit atau hanya 1 kelas di split): {e}")
Â  Â  Â  Â  Â return None


Â  Â  # TF-IDF Vectorization
Â  Â  st.write("Membuat fitur TF-IDF...")
Â  Â  # Limit features for performance, can be adjusted
Â  Â  tfidf_vectorizer = TfidfVectorizer(max_features=1000)
Â  Â  X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)
Â  Â  X_test_tfidf = tfidf_vectorizer.transform(X_test)
Â  Â  st.write(f"Bentuk matriks TF-IDF (Train): {X_train_tfidf.shape}")

Â  Â  results = {}

Â  Â  # Naive Bayes
Â  Â  st.write("Melatih Naive Bayes...")
Â  Â  nb_model = MultinomialNB(alpha=0.2) # Alpha from notebook
Â  Â  nb_model.fit(X_train_tfidf, y_train)
Â  Â  y_pred_nb = nb_model.predict(X_test_tfidf)
Â  Â  nb_accuracy = accuracy_score(y_test, y_pred_nb)
Â  Â  nb_report = classification_report(y_test, y_pred_nb, output_dict=True, zero_division=0)
Â  Â  nb_cm = confusion_matrix(y_test, y_pred_nb, labels=nb_model.classes_)
Â  Â  results['naive_bayes'] = {'accuracy': nb_accuracy, 'report': nb_report, 'cm': nb_cm, 'labels': nb_model.classes_}
Â  Â  st.write(f"Akurasi Naive Bayes: {nb_accuracy*100:.2f}%")

Â  Â  # SVM
Â  Â  st.write("Melatih SVM (Linear)...")
Â  Â  svm_model = SVC(kernel='linear', random_state=42)
Â  Â  svm_model.fit(X_train_tfidf, y_train)
Â  Â  y_pred_svm = svm_model.predict(X_test_tfidf)
Â  Â  svm_accuracy = accuracy_score(y_test, y_pred_svm)
Â  Â  svm_report = classification_report(y_test, y_pred_svm, output_dict=True, zero_division=0)
Â  Â  svm_cm = confusion_matrix(y_test, y_pred_svm, labels=svm_model.classes_) # Use model classes for label order
Â  Â  results['svm'] = {'accuracy': svm_accuracy, 'report': svm_report, 'cm': svm_cm, 'labels': svm_model.classes_}
Â  Â  st.write(f"Akurasi SVM: {svm_accuracy*100:.2f}%")

Â  Â  st.success("Pelatihan dan evaluasi model ML selesai.")
Â  Â  return results


# ==============================================================================
# Single Text Processing Function
# ==============================================================================
def process_single_text(input_text, _senti_dict, _sorted_idioms, _negating_words, _question_words):
Â  Â  if not input_text or not isinstance(input_text, str) or not input_text.strip():
Â  Â  Â  Â  return None, "Input teks kosong atau tidak valid."
Â  Â  cleaned = clean_text(input_text)
Â  Â  if not cleaned: return None, "Teks menjadi kosong setelah cleaning."
Â  Â  case_folded = cleaned.lower()
Â  Â  sentiment = label_sentiment_canggih(case_folded, _senti_dict, _sorted_idioms, _negating_words, _question_words)
Â  Â  return sentiment, case_folded

# ==============================================================================
# Helper Function for Download
# ==============================================================================
@st.cache_data
def convert_df_to_csv(df):
Â  Â return df.to_csv(index=False).encode('utf-8')

# ! ! ! FUNGSI BARU UNTUK WORDCLOUD ! ! !
@st.cache_data
def generate_wordcloud(_df, sentiment_label, _stop_words):
Â  Â  """Membuat dan mengembalikan array gambar Word Cloud."""
Â  Â  st.info(f"Membuat word cloud untuk sentimen '{sentiment_label}'...")
Â  Â  text_data = _df[_df['sentiment'] == sentiment_label]['case_folded_text']
Â  Â Â 
Â  Â  if text_data.empty:
Â  Â  Â  Â  st.write(f"Tidak ada data '{sentiment_label}' untuk Word Cloud.")
Â  Â  Â  Â  return None
Â  Â  Â  Â Â 
Â  Â  all_text = " ".join(text for text in text_data)
Â  Â Â 
Â  Â  if not all_text.strip():
Â  Â  Â  Â  st.write(f"Tidak ada teks valid '{sentiment_label}' untuk Word Cloud.")
Â  Â  Â  Â  return None
Â  Â Â 
Â  Â  try:
Â  Â  Â  Â  # Tentukan colormap berdasarkan sentimen
Â  Â  Â  Â  colormap = 'Greens' if sentiment_label == 'positif' else ('Reds' if sentiment_label == 'negatif' else 'Greys')
Â  Â  Â  Â Â 
Â  Â  Â  Â  wordcloud = WordCloud(width=800, height=400,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  background_color='white',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stopwords=_stop_words,Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  colormap=colormap,Â  # Menggunakan colormap yang dipilih
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  min_font_size=10,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  prefer_horizontal=0.9).generate(all_text)
Â  Â  Â  Â Â 
Â  Â  Â  Â  return wordcloud.to_array() # Mengembalikan array untuk st.image
Â  Â Â 
Â  Â  except Exception as e:
Â  Â  Â  Â  st.warning(f"Gagal membuat word cloud untuk '{sentiment_label}': {e}")
Â  Â  Â  Â  return None


# ==============================================================================
# Streamlit UI
# ==============================================================================

st.set_page_config(layout="wide", page_title="Analisis Sentimen Lexicon+ML")

st.title("ðŸ“Š Aplikasi Analisis Sentimen (Lexicon + ML)")
st.markdown("""
Aplikasi ini melakukan preprocessing, pelabelan sentimen berbasis **Lexicon** (InSet & SentiStrength),
dan melatih model **Machine Learning** (Naive Bayes & SVM) menggunakan TF-IDF pada data yang diunggah.

Pilih metode input: **Unggah File** (untuk lexicon & ML) atau **Input Teks Langsung** (hanya lexicon).
""")

# --- Load Resources ---
with st.spinner("â³ Memuat sumber daya (kamus, model)..."):
Â  Â  # (URLs remain the same)
Â  Â  url_kamus_norm = 'https://raw.githubusercontent.com/onpilot/sentimen-bahasa/master/kamus/nasalsabila_kamus-alay/_json_colloquial-indonesian-lexicon.txt'
Â  Â  url_stopwords_online = 'https://raw.githubusercontent.com/onpilot/sentimen-bahasa/master/kamus/masdevid_id-stopwords/id.stopwords.02.01.2016.txt'
Â  Â  lexicon_urls = { 'inset_fajri_pos': 'https://raw.githubusercontent.com/fajri91/InSet/master/positive.tsv', 'inset_fajri_neg': 'https://raw.githubusercontent.com/fajri91/InSet/master/negative.tsv', 'inset_onpilot_pos': 'https://raw.githubusercontent.com/onpilot/sentimen-bahasa/master/leksikon/inset/positive.tsv', 'inset_onpilot_neg': 'https://raw.githubusercontent.com/onpilot/sentimen-bahasa/master/leksikon/inset/negative.tsv', 'senti_json': 'https://raw.githubusercontent.com/onpilot/sentimen-bahasa/master/leksikon/sentistrength_id/_json_sentiwords_id.txt', 'booster': 'https://raw.githubusercontent.com/onpilot/sentimen-bahasa/master/leksikon/sentistrength_id/boosterwords_id.txt', 'emoticon': 'https://raw.githubusercontent.com/onpilot/sentimen-bahasa/master/leksikon/sentistrength_id/emoticon_id.txt', 'idiom': 'https://raw.githubusercontent.com/onpilot/sentimen-bahasa/master/leksikon/sentistrength_id/idioms_id.txt', 'negating': 'https://raw.githubusercontent.com/onpilot/sentimen-bahasa/master/leksikon/sentistrength_id/negatingword.txt', 'question': 'https://raw.githubusercontent.com/onpilot/sentimen-bahasa/master/leksikon/sentistrength_id/questionword.txt' }
Â  Â Â 
Â  Â  # Muat semua resource
Â  Â  kamus_normalisasi = load_kamus_normalisasi(url_kamus_norm)
Â  Â  stop_words = load_combined_stopwords(url_stopwords_online) # <--- stop_words dimuat di sini
Â  Â  stemmer = get_stemmer()
Â  Â  senti_dict, sorted_idioms, negating_words, question_words = load_all_sentiment_dictionaries(lexicon_urls)
Â  Â Â 
Â  Â  try: nltk.data.find('tokenizers/punkt')
Â  Â  except LookupError: st.info("Mengunduh NLTK Punkt..."); nltk.download('punkt')

st.success("âœ… Sumber daya siap.")
st.markdown("---")

# --- Tabs ---
tab1, tab2 = st.tabs(["ðŸ“¤ Unggah File (Lexicon + ML)", "âŒ¨ï¸ Input Teks Langsung (Hanya Lexicon)"])

# --- Tab 1: File Upload ---
with tab1:
Â  Â  st.header("1. Unggah Data Anda (CSV/Excel)")
Â  Â  uploaded_file = st.file_uploader("Pilih file", type=['csv', 'xlsx'], label_visibility="collapsed", key="file_uploader") # <-- Kunci ini sekarang unik

Â  Â  # Initialize session state for results if not present
Â  Â  if 'labeled_df' not in st.session_state:
Â  Â  Â  Â  st.session_state['labeled_df'] = None
Â  Â  if 'ml_results' not in st.session_state:
Â  Â  Â  Â  st.session_state['ml_results'] = None
Â  Â  if 'processed_filename' not in st.session_state:
Â  Â  Â  Â  st.session_state['processed_filename'] = ""


Â  Â  if uploaded_file is not None:
Â  Â  Â  Â  # Clear previous results if a new file is uploaded
Â  Â  Â  Â  if uploaded_file.name != st.session_state.get('processed_filename', ""):
Â  Â  Â  Â  Â  Â  Â st.session_state['labeled_df'] = None
Â  Â  Â  Â  Â  Â  Â st.session_state['ml_results'] = None
Â  Â  Â  Â  Â  Â  Â st.session_state['processed_filename'] = uploaded_file.name


Â  Â  Â  Â  try:
Â  Â  Â  Â  Â  Â  if uploaded_file.name.endswith('.csv'): df_input = pd.read_csv(uploaded_file)
Â  Â  Â  Â  Â  Â  else: df_input = pd.read_excel(uploaded_file, engine='openpyxl')

Â  Â  Â  Â  Â  Â  st.success(f"File '{uploaded_file.name}' diunggah ({len(df_input)} baris).")
Â  Â  Â  Â  Â  Â  st.dataframe(df_input.head(), hide_index=True)

Â  Â  Â  Â  Â  Â  st.header("2. Pilih Kolom Teks")
Â  Â  Â  Â  Â  Â  available_columns = [""] + df_input.columns.tolist()
Â  Â  Â  Â  Â  Â  text_column = st.selectbox("Pilih kolom:", options=available_columns, index=0, key="selectbox_column")

Â  Â  Â  Â  Â  Â  if text_column:
Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"Kolom teks: **{text_column}**")

Â  Â  Â  Â  Â  Â  Â  Â  # --- Preprocessing & Lexicon Labeling Button ---
Â  Â  Â  Â  Â  Â  Â  Â  st.header("3. Proses Preprocessing & Labeling Lexicon")
Â  Â  Â  Â  Â  Â  Â  Â  if st.button("ðŸ”¬ Proses Teks & Label Lexicon", key="button_process_lexicon"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['labeled_df'] = None # Clear previous labeling result
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['ml_results'] = None # Clear previous ML result

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if text_column not in df_input.columns: st.error("Kolom tidak valid.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  elif df_input[text_column].isnull().all(): st.error(f"Kolom '{text_column}' kosong.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if not pd.api.types.is_string_dtype(df_input[text_column]):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â try: df_input[text_column] = df_input[text_column].astype(str); st.warning(f"Kolom dikonversi ke teks.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â except Exception as e: st.error(f"Gagal konversi: {e}"); st.stop()

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_valid = df_input.dropna(subset=[text_column])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_valid = df_valid[df_valid[text_column].astype(str).str.strip().astype(bool)]

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if df_valid.empty: st.warning("Tidak ada teks valid.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("â³ Memproses & melabel (lexicon)..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_labeled_lexicon = preprocess_and_label_df(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  df_valid, text_column,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  senti_dict, sorted_idioms, negating_words, question_words
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if df_labeled_lexicon.empty: st.warning("Tidak ada hasil labeling.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['labeled_df'] = df_labeled_lexicon # Store result

Â  Â  Â  Â  Â  Â  Â  Â  # --- Display Lexicon Labeling Results ---
Â  Â  Â  Â  Â  Â  Â  Â  if st.session_state['labeled_df'] is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success("ðŸŽ‰ Preprocessing & Labeling Lexicon Selesai!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Hasil Labeling Lexicon (Contoh):")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(st.session_state['labeled_df'][[text_column, 'case_folded_text', 'sentiment']].head(20), hide_index=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Distribusi Sentimen (Lexicon):")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sentiment_counts_lex = st.session_state['labeled_df']['sentiment'].value_counts()
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.bar_chart(sentiment_counts_lex)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write(sentiment_counts_lex)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ! ! ! BAGIAN BARU UNTUK WORDCLOUD ! ! !
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Word Clouds Sentimen (Lexicon):")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write("Word cloud dibuat dari kolom 'case_folded_text' berdasarkan label lexicon.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  col_wc1, col_wc2 = st.columns(2)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col_wc1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("<h5>Positif ðŸ˜Š</h5>", unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Membuat Word Cloud Positif..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Memanggil fungsi generate_wordcloud
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wc_pos_array = generate_wordcloud(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['labeled_df'],Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'positif',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stop_words # <--- Menggunakan stop_words yang sudah dimuat
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if wc_pos_array is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.image(wc_pos_array, use_column_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write("Tidak ada data positif untuk Word Cloud.")

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col_wc2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("<h5>Negatif ðŸ˜ </h5>", unsafe_allow_html=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with st.spinner("Membuat Word Cloud Negatif..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â # Memanggil fungsi generate_wordcloud
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  wc_neg_array = generate_wordcloud(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.session_state['labeled_df'],Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  'negatif',Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  stop_words # <--- Menggunakan stop_words yang sudah dimuat
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if wc_neg_array is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.image(wc_neg_array, use_column_width=True)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.write("Tidak ada data negatif untuk Word Cloud.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.markdown("---") # Pemisah sebelum ML
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # ! ! ! AKHIR BAGIAN BARU WORDCLOUD ! ! !


Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- ML Modeling Button ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.header("4. Latih & Evaluasi Model ML (Naive Bayes & SVM)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.warning("Model ML akan dilatih menggunakan kolom 'sentiment' hasil lexicon sebagai target.")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # Slider Test Size (dari permintaan Anda sebelumnya)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  selected_test_size = st.slider(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label="Pilih Test Size (Proporsi Data Uji):",
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  min_value=0.1,Â  # 10%
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  max_value=0.9,Â  # 90%
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  value=0.3,Â  Â  Â  # Default 30%
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  step=0.1,Â  Â  Â  Â # Kelipatan 10%
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  key="slider_test_size"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.info(f"Data uji akan menggunakan {int(selected_test_size*100)}% dari total data. (Data Latih: {int((1-selected_test_size)*100)}%)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.button("ðŸ¤– Latih Model ML", key="button_train_ml"):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state['ml_results'] = None # Clear previous ML results
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â with st.spinner("â³ Melatih & mengevaluasi model ML..."):
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â ml_results_dict = train_and_evaluate_models(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state['labeled_df'],
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â selected_test_size=selected_test_size # <-- Menggunakan nilai dari slider
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â )
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â 
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â if ml_results_dict:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state['ml_results'] = ml_results_dict # Store results


Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- Display ML Results ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  if st.session_state['ml_results'] is not None:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.success("ðŸŽ‰ Pelatihan & Evaluasi Model ML Selesai!")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  results_ml = st.session_state['ml_results']

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Hasil Evaluasi Model:")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  col1, col2 = st.columns(2)
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col1:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Akurasi Naive Bayes", f"{results_ml['naive_bayes']['accuracy']:.2%}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("Laporan Klasifikasi NB:")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(pd.DataFrame(results_ml['naive_bayes']['report']).transpose())
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  with col2:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.metric("Akurasi SVM (Linear)", f"{results_ml['svm']['accuracy']:.2%}")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.text("Laporan Klasifikasi SVM:")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.dataframe(pd.DataFrame(results_ml['svm']['report']).transpose())

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.subheader("Confusion Matrix:")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sns.heatmap(results_ml['naive_bayes']['cm'], annot=True, fmt='d', cmap='Blues', ax=ax1,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  xticklabels=results_ml['naive_bayes']['labels'], yticklabels=results_ml['naive_bayes']['labels'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax1.set_title('Naive Bayes')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax1.set_xlabel('Predicted')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax1.set_ylabel('Actual')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  sns.heatmap(results_ml['svm']['cm'], annot=True, fmt='d', cmap='Oranges', ax=ax2,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  xticklabels=results_ml['svm']['labels'], yticklabels=results_ml['svm']['labels'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax2.set_title('SVM (Linear)')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax2.set_xlabel('Predicted')
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  ax2.set_ylabel('Actual')

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.pyplot(fig)

Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  # --- Download Button for Labeled Data ---
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.header("5. Unduh Hasil (Termasuk Label Lexicon)")
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  csv_data = convert_df_to_csv(st.session_state['labeled_df'])
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  output_filename = f"hasil_sentimen_lexicon_{uploaded_file.name.split('.')[0]}.csv"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  st.download_button(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  label="ðŸ“¥ Unduh Data + Label Lexicon (.csv)", data=csv_data,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  file_name=output_filename, mime='text/csv', key="download_button"
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  Â  Â  elif not text_column:
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â st.warning("â˜ï¸ Pilih kolom teks untuk memulai proses.")


Â  Â  Â  Â  except Exception as e:
Â  Â  Â  Â  Â  Â  st.error(f"Error membaca atau memproses file: {e}")

Â  Â  else:
Â  Â  Â  Â  st.info("Unggah file CSV atau Excel di atas untuk memulai.")


# --- Tab 2: Text Input ---
with tab2:
Â  Â  st.header("Analisis Teks Tunggal (Hanya Lexicon)")
Â  Â  st.info("Fitur ini hanya menggunakan metode berbasis kamus (lexicon), tidak melatih model ML.")
Â  Â  user_text = st.text_area("Ketik atau paste teks Anda:", height=150, key="text_area_input")

Â  Â  if st.button("ðŸš€ Analisis Teks Ini!", key="button_process_text"):
Â  Â  Â  Â  if user_text and user_text.strip():
Â  Â  Â  Â  Â  Â  with st.spinner("â³ Menganalisis teks..."):
Â  Â  Â  Â  Â  Â  Â  Â  hasil_sentimen, teks_diproses = process_single_text(
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  user_text, senti_dict, sorted_idioms, negating_words, question_words
Â  Â  Â  Â  Â  Â  Â  Â  )

Â  Â  Â  Â  Â  Â  st.subheader("Hasil Analisis Teks:")
Â  Â  Â  Â  Â  Â  if hasil_sentimen:
Â  Â  Â  Â  Â  Â  Â  Â  Â st.write("Teks setelah Cleaning & Case Folding:")
Â  Â  Â  Â  Â  Â  Â  Â  Â st.info(f"`{teks_diproses}`")
Â  Â  Â  Â  Â  Â  Â  Â  Â st.write("Hasil Sentimen (Lexicon):")
Â  Â  Â  Â  Â  Â  Â  Â  Â if hasil_sentimen == 'positif': st.success(f"**{hasil_sentimen.upper()}** ðŸ˜Š")
Â  Â  Â  Â  Â  Â  Â  Â  Â elif hasil_sentimen == 'negatif': st.error(f"**{hasil_sentimen.upper()}** ðŸ˜ ")
Â  Â  Â  Â  Â  Â  Â  Â  Â else: st.warning(f"**{hasil_sentimen.upper()}** ðŸ˜")
Â  Â  Â  Â  Â  Â  Â  Â  Â st.session_state['processed_text_result'] = {'text': user_text, 'processed': teks_diproses, 'sentiment': hasil_sentimen}
Â  Â  Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  Â  Â  Â st.error(f"Gagal: {teks_diproses}")
Â  Â  Â  Â  else:
Â  Â  Â  Â  Â  Â  st.warning("â˜ï¸ Masukkan teks terlebih dahulu.")

# --- Footer ---
st.markdown("---")
st.markdown("Dibuat dengan Streamlit")
